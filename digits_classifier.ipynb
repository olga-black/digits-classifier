{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, TensorDataset, DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset and split it into the train, eval, and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(root='./',\n",
    "                transform=transforms.ToTensor(),\n",
    "                download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = MNIST(root='./',\n",
    "                train=False,\n",
    "                transform=transforms.ToTensor(),\n",
    "                download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = dataset[2][0][-1, :, :]\n",
    "label = dataset[2][1]\n",
    "\n",
    "plt.imshow(image, cmap='winter_r')\n",
    "plt.xlabel(f\"This digit is a {label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[2][1] == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_targets(dataset):\n",
    "    \"\"\"\n",
    "    Change the targets to binary\n",
    "    Digit 4 -> 1\n",
    "    All other digits -> 0\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset.targets[dataset.targets != 4] = 0\n",
    "    dataset.targets[dataset.targets == 4] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_targets(dataset)\n",
    "binary_targets(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = test_set[4][0][-1, :, :]\n",
    "label = bool(test_set[4][1])\n",
    "\n",
    "plt.imshow(image, cmap='winter_r')\n",
    "plt.xlabel(f\"This digit is a 4: {label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The available MNIST dataset does not contain a validation set.\n",
    "To prevent overfitting, I will create a validation set by slicing 10 000 images from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = torch.arange(0, len(dataset)-10000)\n",
    "val_indices = torch.arange(len(dataset)-10000, len(dataset))\n",
    "train_set = Subset(dataset, train_indices)\n",
    "val_set = Subset(dataset, val_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom multi-image dataset from MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average 10\n",
    "images with standard deviation of 3, but no less than 3 and no more than 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_shape(size, mean=10, sd=3, low=3, upp=30):\n",
    "    \"\"\"\n",
    "    In: desired dataset size, mean, standard deviation, lower and upper boundary\n",
    "    Out: tuple of the given size, where each item is a randomly generated sequence length\n",
    "    \"\"\"\n",
    "    \n",
    "    trunc = truncnorm((low - mean) / sd, (upp - mean) / sd, loc=mean, scale=sd)\n",
    "    trunc_array = torch.round(torch.from_numpy(trunc.rvs(size))).int()\n",
    "    trunc_list = [int(num) for num in trunc_array]\n",
    "    \n",
    "    return tuple(trunc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shape = create_dataset_shape(4940)\n",
    "val_shape = create_dataset_shape(960)\n",
    "test_shape = create_dataset_shape(960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sum(train_shape) <= len(train_set)\n",
    "assert sum(val_shape) <= len(val_set)\n",
    "assert sum(test_shape) <= len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_x_and_y(dataset):\n",
    "    \"\"\"\n",
    "    Split inputs and targets in order to modify them into a custom dataset\n",
    "    In: torch dataset\n",
    "    Out: inputs (torch tensor), targets (tuple)\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_x, dataset_y = zip(*dataset)\n",
    "    dataset_x = torch.stack(list(dataset_x), dim=0)\n",
    "    dataset_y = torch.tensor(dataset_y)\n",
    "    return dataset_x, dataset_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_dataset(inputs, targets, data_shape):\n",
    "    \"\"\"\n",
    "    In: inputs (tensor) and targets (iterable) from existing dataset;\n",
    "        new desired shape of multi-image dataset\n",
    "    Out: new inputs (len(shape) x max sequence length x channel x image_dim1 x image_dim2) \n",
    "        targets (len(shape))\n",
    "            \n",
    "    \"\"\"\n",
    "    size_x = (len(data_shape), max(data_shape)) + tuple(inputs[0].shape)\n",
    "    print(size_x)                                                   \n",
    "    size_y = (len(data_shape), max(data_shape))\n",
    "    print(size_y)\n",
    "    \n",
    "    x = torch.zeros(size_x)\n",
    "    y = torch.zeros(size_y)\n",
    "\n",
    "    line_index = 0\n",
    "\n",
    "    for idx, dim in enumerate(data_shape):\n",
    "        x[idx,0:dim] = inputs[line_index: line_index+dim]\n",
    "        y[idx,0:dim] = targets[line_index: line_index+dim]\n",
    "        line_index += dim\n",
    "\n",
    "    y = torch.where(y.sum(axis=1)>=1, 1, 0)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4940, 20, 1, 28, 28)\n",
      "(4940, 20)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y = split_x_and_y(train_set)\n",
    "train_x, train_y = custom_dataset(train_x, train_y, train_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960, 19, 1, 28, 28)\n",
      "(960, 19)\n"
     ]
    }
   ],
   "source": [
    "val_x, val_y = split_x_and_y(val_set)\n",
    "val_x, val_y = custom_dataset(val_x, val_y, val_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(960, 19, 1, 28, 28)\n",
      "(960, 19)\n"
     ]
    }
   ],
   "source": [
    "test_x, test_y = split_x_and_y(test_set)\n",
    "test_x, test_y = custom_dataset(test_x, test_y, test_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "val_dataset = TensorDataset(val_x, val_y)\n",
    "test_dataset = TensorDataset(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageClassifier(nn.Module):\n",
    "        \n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 seq_length,\n",
    "                 dropout_rate,\n",
    "                 n_outputs,\n",
    "                 batch_size\n",
    "                ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 64, 4)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 4)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.fc = nn.Linear(256, n_outputs)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, inputs): \n",
    "        x_tmp = inputs.reshape(batch_size*seq_length, 1, input_size, input_size)\n",
    "        x = self.maxpool(self.relu(self.conv1(x_tmp)))\n",
    "        x = self.maxpool(self.relu(self.conv2(x)))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = x.reshape(batch_size, seq_length, 256)\n",
    "        x = self.dropout(x)\n",
    "        x = x.mean(dim=1) \n",
    "        x = self.fc(x)\n",
    "        x = self.sigmoid(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28\n",
    "seq_length = 20\n",
    "n_outputs = 1\n",
    "dropout_rate = 0.25\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassifier(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (relu): ReLU()\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = ImageClassifier(\n",
    "                        input_size=input_size,\n",
    "                        seq_length=seq_length,\n",
    "                        dropout_rate=dropout_rate,\n",
    "                        n_outputs=n_outputs,\n",
    "                        batch_size=batch_size\n",
    "                        ).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size)\n",
    "test_loader = DataLoader(val_dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 20, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for i, (inputs, targets)in enumerate(train_loader):\n",
    "    print(inputs.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_loader=train_loader,\n",
    "                val_loader=val_loader,\n",
    "                model=model,\n",
    "                criterion=nn.BCELoss(),\n",
    "                num_epochs=num_epochs,\n",
    "                learning_rate=learning_rate\n",
    "               ):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        model.train()\n",
    "        \n",
    "        for inputs, targets in train_loader: \n",
    "            inputs = inputs.to(device=device)\n",
    "            targets = targets.to(device=device)\n",
    "            \n",
    "            optimizer= torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            output = model(inputs)\n",
    "            loss = criterion(output.squeeze(), targets.float())\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            \n",
    "        train_losses.append(total_loss/len(train_loader))\n",
    "        model.eval()\n",
    "        \n",
    "        \n",
    "        for val_inputs, val_labels in val_loader:\n",
    "            val_output = model(val_inputs)\n",
    "            val_loss = criterion(val_output.squeeze(), val_labels.float())\n",
    "            total_loss_val += val_loss.item()\n",
    "            \n",
    "            \n",
    "        val_losses.append(total_loss_val/len(val_loader))\n",
    "    \n",
    "        \n",
    "        print(f\"Epoch: {epoch+1}/{ n_epochs}\\t\",\n",
    "              f\"{((time.time() - start_time) / 60):.2f} min\\t\",\n",
    "              f\"Loss: {total_loss/len(train_loader):.3f}\\t\",\n",
    "              f\"Val Loss: {total_loss_val/len(val_loader):.3f}\\t\")\n",
    "            \n",
    "            \n",
    "    print(f\"Training completed in {(time.time() - start_time) / 60} min.\")\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1280, 1, 28, 28]' is invalid for input of size 188160",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-53eadfe4dacd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-fc241d056e9f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader, model, criterion, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-a2ef7f4410de>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mx_tmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1280, 1, 28, 28]' is invalid for input of size 188160"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = train_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
